{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate #用于 PromptTemplate 为字符串提示创建模板\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate #for multiple prompt merging\n",
    "\n",
    "#base class of all prompt generators, provide single prompt given arguments\n",
    "class PromptGenerator:\n",
    "    def __init__(self, promptFString):\n",
    "        self.prompt_template = PromptTemplate.from_template(\n",
    "            promptFString\n",
    "            #Sample: \"Tell me a {adjective} joke about {content}.\"\n",
    "        )\n",
    "    \n",
    "    def generatePrompt(self,**kwargs):\n",
    "        formatted_prompt = self.prompt_template.format(**kwargs)\n",
    "        return formatted_prompt\n",
    "\n",
    "\n",
    "#prompt generator for MAD situation:\n",
    "class MADPromptGenerator:\n",
    "    def __init__(self, prePromptGuide, postPromptGuide, numOfAgents):\n",
    "        self.prePromptGuide = PromptTemplate.from_template( prePromptGuide)\n",
    "        self.postPromptGuide = PromptTemplate.from_template(postPromptGuide)\n",
    "        self.numOfAgents = numOfAgents\n",
    "        self.angentsAnsGuide = PromptTemplate.from_template(\"\\n\\n\".join( [f\" One agent response:{{example{index}}}\" for index in range(numOfAgents)]))\n",
    "\n",
    "    def generatePrompt(self, prevAgentsInputs, currentAgentIndex,currentRound, preInputs=None,postInputs=None):\n",
    "        if preInputs !=None:\n",
    "            prePrompt = self.prePromptGuide.format(preInputs)\n",
    "        else:\n",
    "            prePrompt =  self.prePromptGuide.format()\n",
    "        \n",
    "        if postInputs != None:\n",
    "            postPrompt = self.postPromptGuide.format(postInputs)\n",
    "        else:\n",
    "            postPrompt = self.postPromptGuide.format()\n",
    "        \n",
    "        \n",
    "        fullPrompt = prePrompt + \"\\n \\n\"\n",
    "        for i in range(currentRound):\n",
    "            currentAgentInput = prevAgentsInputs[i][\"example{}\".format(currentAgentIndex)]\n",
    "            fullPrompt = fullPrompt +  f\" {currentAgentInput}\" + \"\\n\\n\"\n",
    "            prevAgentsInputs[i][\"example{}\".format(currentAgentIndex)] = \" \"\n",
    "            currentRoundAgentsContext = self.angentsAnsGuide.format(**(prevAgentsInputs[i]))\n",
    "            fullPrompt = fullPrompt + \"These are the recent/updated opinions from other agents:\"+ currentRoundAgentsContext + \"\\n \\n\"\n",
    "            prevAgentsInputs[i][\"example{}\".format(currentAgentIndex)] = currentAgentInput\n",
    "        \n",
    "        fullPrompt += postPrompt\n",
    "\n",
    "        return fullPrompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return type definition\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class MathAns(BaseModel):\n",
    "    DetailedExplainationsToConvinceOthers: str = Field(description=\"detailed record of explainations, should be as detailed and lengthy (at least 100 words) as possible in order to convince others\")\n",
    "    answer: float = Field(description=\"Final Answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser, RetryWithErrorOutputParser\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "import time\n",
    "\n",
    "openaiApiKey = ''\n",
    "if openaiApiKey is None:\n",
    "    raise ValueError(\"OpenAI API key not found. Make sure you have an .env file with the key defined.\")\n",
    "model_name =\"gpt-3.5-turbo-0301\" #\"gpt-3.5-turbo-instruct\"#\"gpt-3.5-turbo\"# \"text-davinci-003\"\n",
    "\n",
    "class ProcessHandler:\n",
    "    def __init__(self,promptFString):\n",
    "        self.pG = PromptGenerator(promptFString)\n",
    "        self.llm = OpenAI(temperature=0.1, openai_api_key=openaiApiKey,model_name=model_name)\n",
    "    \n",
    "    def generateAnswer(self, questionPromptString):\n",
    "        formatedQuestion = self.pG.generatePrompt(questionPromptString)\n",
    "        return self.llm(formatedQuestion)\n",
    "    \n",
    "class MADProcessHandler:\n",
    "    def __init__(self,nrounds,nAgents, prePromptString, postPromptString, parser):\n",
    "        self.pG = MADPromptGenerator(prePromptString,postPromptString,nAgents)\n",
    "        self.nrounds = nrounds\n",
    "        self.llm = OpenAI( openai_api_key=openaiApiKey,model_name=model_name)\n",
    "        self.answersRecords = []\n",
    "        self.nAgents = nAgents\n",
    "        self.parser = parser\n",
    "\n",
    "    def generateAnswer(self, questionSting, prePromptString=None, postPromptString=None):\n",
    "        self.answersRecords.clear()\n",
    "        if prePromptString != None:\n",
    "            questionSting += prePromptString\n",
    "        for i in range(self.nrounds):\n",
    "            self.answersRecords.append({})\n",
    "            for j in range(self.nAgents):\n",
    "                prompt = questionSting\n",
    "                if i !=0:\n",
    "                    print(i)\n",
    "                    prompt = prompt + self.pG.generatePrompt(self.answersRecords, j,i)\n",
    "                prompt = prompt+self.parser.get_format_instructions()\n",
    "                print(\"round{}\".format(i), \"agent{} prompt: \\n\".format(j),prompt)\n",
    "                answer = self.llm(prompt)\n",
    "                print(\"round{}\".format(i), \"agent{} answer: \\n\".format(j),answer)\n",
    "                time.sleep(30)\n",
    "                self.answersRecords[i][\"example{}\".format(j)] = answer\n",
    "        \n",
    "        prompt = questionSting+\"\\n\\n\" + \"The following paragraphs are previous answers given by other agents for your reference \\n\\n\"\n",
    "        for j in range(self.nAgents):\n",
    "            prompt = prompt+ \"agent{}'s answer:\\n\".format(j)+self.answersRecords[-1][\"example{}\".format(j)]+\"\\n\\n\"\n",
    "        prompt = prompt + \"Given the answers from other agents, please draw a final conclusion. Please Think Step by Step. Make sure to state your answer at the end of the response.\"\n",
    "\n",
    "        prompt = prompt+\"\\n\"+self.parser.get_format_instructions()\n",
    "        answer = self.llm(prompt)\n",
    "        \n",
    "        answer = self.parser.parse(answer)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "correct_answer = []\n",
    "#question number 1\n",
    "questions.append('''A boat is acted on by a river current flowing north and by wind blowing on its sails. The boat travels northeast. In which direction is the wind most likely applying force to the sails of the boat?''') #Mercury_7041860\n",
    "correct_answer.append('East')\n",
    "#question number 2\n",
    "questions.append('''In a group of 10 people, each one has 7 friends among the other nine people. Prove that there exist 4 people who are friends of each other.''')\n",
    "correct_answer.append('''Consider an edge between two people means friendship between\n",
    "them. Every node is connected to 7 other nodes. Consider two\n",
    "nodes A, B that are already friend of each other. Out of the remain-\n",
    "ing 8 people, A, B are each connected to 6 of them. So based on\n",
    "pigeonhole principle, They have at least 4 mutual friends. Consider\n",
    "this 4 mutual friends to be x1, x2, x3, x4. If there exists any edge\n",
    "between any pair of them we are done because we will have 4 people\n",
    "who are pairwise friend of each other. So there must be no edge\n",
    "among any pair of them. Now consider any of those nodes. It has\n",
    "to be connected to 7 nodes. However, Now maximum degree of that\n",
    "node is 6 as it can not be connected to 3 nodes (and itself ), which is\n",
    "a contradiction. So it means there exists 4 people that are pairwise\n",
    "friend of each other.\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No startegy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ronaldleung/miniconda3/envs/MAD/lib/python3.11/site-packages/langchain_community/llms/openai.py:248: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIChat\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#no strategy test\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mopenaiApiKey,model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[1;32m      6\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m question       \n",
      "File \u001b[0;32m~/miniconda3/envs/MAD/lib/python3.11/site-packages/langchain_community/llms/openai.py:253\u001b[0m, in \u001b[0;36mBaseOpenAI.__new__\u001b[0;34m(cls, **data)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    246\u001b[0m     model_name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m model_name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mgpt-4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    247\u001b[0m ) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m-instruct\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_name:\n\u001b[1;32m    248\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    249\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to use a chat model. This way of initializing it is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mno longer supported. Instead, please use: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`from langchain_community.chat_models import ChatOpenAI`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m     )\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mreturn\u001b[39;00m OpenAIChat(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdata)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/MAD/lib/python3.11/site-packages/langchain_core/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/MAD/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAIChat\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "#no strategy test\n",
    "\n",
    "llm = OpenAI(temperature=0.5, openai_api_key=openaiApiKey,model_name=model_name)\n",
    "\n",
    "for question in questions:\n",
    "    prompt = question       \n",
    "    ans = llm(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm2 = OpenAI(temperature=0.5, openai_api_key=openaiApiKey,model_name=model_name)\n",
    "\n",
    "for question in questions:\n",
    "    prompt = question + \"Please think step by step.\"    \n",
    "    ans = llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MAD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = 2\n",
    "rounds = 2\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MathAns)\n",
    "ph = MADProcessHandler(rounds,agents,\n",
    "     \" Make sure to state your answer at the end of the response.\"\n",
    "     ,\"Use these opinions carefully as additional advice, can you provide an updated answer? Make sure to state your answer at the end of the response.\",\n",
    "    parser)\n",
    "\n",
    "for question in questions:\n",
    "    prompt = question + \"Please think step by step.\"    \n",
    "    ans = ph.generateAnswer(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('MAD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11dedc26855adb07ba7a05365f8b8006f26af4cb666f158a6ec1788586e3085c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
